import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.optim as optim

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False


class LinearRegressionModel(nn.Module):
    """çº¿æ€§å›å½’æ¨¡å‹ y = wx + b"""

    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)  # è¾“å…¥ç»´åº¦1ï¼Œè¾“å‡ºç»´åº¦1

        # åˆå§‹åŒ–æƒé‡å’Œåç½®ä¸ºæ­£æ€åˆ†å¸ƒ
        nn.init.normal_(self.linear.weight, mean=0.0, std=0.1)
        nn.init.normal_(self.linear.bias, mean=0.0, std=0.1)

    def forward(self, x):
        return self.linear(x)


def train_model(x, y, optimizer_type='sgd', learning_rate=0.01, epochs=1000):
    """è®­ç»ƒæ¨¡å‹å¹¶è¿”å›å†å²è®°å½•ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–å™¨"""
    # è½¬æ¢ä¸ºå¼ é‡
    x_tensor = torch.FloatTensor(x).view(-1, 1)
    y_tensor = torch.FloatTensor(y).view(-1, 1)

    # æ ‡å‡†åŒ–æ•°æ®
    x_mean, x_std = torch.mean(x_tensor), torch.std(x_tensor)
    y_mean, y_std = torch.mean(y_tensor), torch.std(y_tensor)
    x_normalized = (x_tensor - x_mean) / x_std
    y_normalized = (y_tensor - y_mean) / y_std

    # åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
    model = LinearRegressionModel()
    criterion = nn.MSELoss()

    # é€‰æ‹©ä¼˜åŒ–å™¨
    if optimizer_type == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'adagrad':
        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'adamax':
        optimizer = optim.Adamax(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'asgd':
        optimizer = optim.ASGD(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'rmsprop':
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'rprop':
        optimizer = optim.Rprop(model.parameters(), lr=learning_rate)
    elif optimizer_type == 'lbfgs':
        # LBFGSéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè®¾ç½®æ›´å¤šå‚æ•°
        optimizer = optim.LBFGS(model.parameters(), lr=learning_rate,
                                history_size=100, max_iter=4)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

    # è®°å½•è®­ç»ƒå†å²
    loss_history = []
    w_history = []
    b_history = []

    # è®­ç»ƒæ¨¡å‹
    for epoch in range(epochs):
        # å¯¹äºLBFGSä¼˜åŒ–å™¨éœ€è¦ç‰¹æ®Šçš„closureå‡½æ•°
        if optimizer_type == 'lbfgs':
            def closure():
                optimizer.zero_grad()
                y_pred = model(x_normalized)
                loss = criterion(y_pred, y_normalized)
                loss.backward()
                return loss

            loss = optimizer.step(closure)
            loss_val = loss.item()
        else:
            # å‰å‘ä¼ æ’­
            y_pred = model(x_normalized)
            loss = criterion(y_pred, y_normalized)
            loss_val = loss.item()

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # è®°å½•å½“å‰çŠ¶æ€
        loss_history.append(loss_val)
        w = model.linear.weight.item()
        b = model.linear.bias.item()
        w_history.append(w)
        b_history.append(b)

        # æ¯100è½®æ‰“å°ä¿¡æ¯
        if (epoch + 1) % 100 == 0:
            print(f"{optimizer_type} - è½®æ¬¡ {epoch + 1}/{epochs}, æŸå¤±: {loss_val:.6f}, w: {w:.6f}, b: {b:.6f}")

    # è½¬æ¢å›åŸå§‹å°ºåº¦å‚æ•°
    w_original = w * (y_std / x_std).item()
    b_original = (b * y_std + y_mean - w * (y_std / x_std) * x_mean).item()

    return model, w_original, b_original, loss_history, w_history, b_history


def compare_optimizers(x, y, optimizers, learning_rate=0.01, epochs=1000):
    """æ¯”è¾ƒå¤šç§ä¼˜åŒ–å™¨çš„æ€§èƒ½"""
    results = {}

    # è®­ç»ƒæ¯ç§ä¼˜åŒ–å™¨
    for opt in optimizers:
        print(f"\n===== å¼€å§‹ä½¿ç”¨ {opt} ä¼˜åŒ–å™¨è®­ç»ƒ =====")
        model, w, b, loss_hist, w_hist, b_hist = train_model(
            x, y, optimizer_type=opt, learning_rate=learning_rate, epochs=epochs)
        results[opt] = {
            'model': model,
            'w': w,
            'b': b,
            'loss_history': loss_hist,
            'w_history': w_hist,
            'b_history': b_hist
        }

    # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
    plot_comparison_results(x, y, results, epochs)

    return results


def plot_comparison_results(x, y, results, epochs):
    """å¯è§†åŒ–ä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½æ¯”è¾ƒ"""
    optimizers = list(results.keys())
    num_optimizers = len(optimizers)

    # åˆ›å»ºç”»å¸ƒ
    plt.figure(figsize=(20, 16))

    # 1. æ‰€æœ‰ä¼˜åŒ–å™¨çš„æŸå¤±æ›²çº¿å¯¹æ¯”
    plt.subplot(2, 2, 1)
    for opt in optimizers:
        plt.plot(range(epochs), results[opt]['loss_history'], label=opt, linewidth=1.5)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('ä¸åŒä¼˜åŒ–å™¨çš„æŸå¤±æ›²çº¿å¯¹æ¯”')
    plt.legend()
    plt.grid(alpha=0.3)

    # 2. æ‰€æœ‰ä¼˜åŒ–å™¨çš„æƒé‡wå˜åŒ–å¯¹æ¯”
    plt.subplot(2, 2, 2)
    for opt in optimizers:
        plt.plot(range(epochs), results[opt]['w_history'], label=opt, linewidth=1.5)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æƒé‡ w')
    plt.title('ä¸åŒä¼˜åŒ–å™¨çš„æƒé‡wå˜åŒ–å¯¹æ¯”')
    plt.legend()
    plt.grid(alpha=0.3)

    # 3. æ‰€æœ‰ä¼˜åŒ–å™¨çš„åç½®bå˜åŒ–å¯¹æ¯”
    plt.subplot(2, 2, 3)
    for opt in optimizers:
        plt.plot(range(epochs), results[opt]['b_history'], label=opt, linewidth=1.5)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('åç½® b')
    plt.title('ä¸åŒä¼˜åŒ–å™¨çš„åç½®bå˜åŒ–å¯¹æ¯”')
    plt.legend()
    plt.grid(alpha=0.3)

    # 4. æ‰€æœ‰ä¼˜åŒ–å™¨çš„æ‹Ÿåˆç»“æœå¯¹æ¯”
    plt.subplot(2, 2, 4)
    plt.scatter(x, y, color='skyblue', label='åŸå§‹æ•°æ®', alpha=0.6)
    for opt in optimizers:
        w = results[opt]['w']
        b = results[opt]['b']
        plt.plot(x, w * x + b, linewidth=2, label=f'{opt}: y = {w:.2f}x + {b:.2f}')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('ä¸åŒä¼˜åŒ–å™¨çš„æ‹Ÿåˆç»“æœå¯¹æ¯”')
    plt.legend()
    plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.show()

    # å•ç‹¬ç»˜åˆ¶æ¯ä¸ªä¼˜åŒ–å™¨çš„è¯¦ç»†å‚æ•°å˜åŒ–å›¾
    for opt in optimizers:
        plt.figure(figsize=(15, 10))

        # å­å›¾1ï¼šæŸå¤±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(range(epochs), results[opt]['loss_history'], color='darkorange', linewidth=1.5)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('æŸå¤±å€¼')
        plt.title(f'{opt} æŸå¤±å€¼éšè®­ç»ƒè½®æ¬¡å˜åŒ–')
        plt.grid(alpha=0.3)

        # å­å›¾2ï¼šwä¸losså…³ç³»
        plt.subplot(2, 2, 2)
        plt.plot(results[opt]['w_history'], results[opt]['loss_history'], color='forestgreen', linewidth=1.5)
        plt.xlabel('æƒé‡ w')
        plt.ylabel('æŸå¤±å€¼')
        plt.title(f'{opt} æƒé‡ w ä¸æŸå¤±çš„å…³ç³»')
        plt.grid(alpha=0.3)

        # å­å›¾3ï¼šbä¸losså…³ç³»
        plt.subplot(2, 2, 3)
        plt.plot(results[opt]['b_history'], results[opt]['loss_history'], color='darkorchid', linewidth=1.5)
        plt.xlabel('åç½® b')
        plt.ylabel('æŸå¤±å€¼')
        plt.title(f'{opt} åç½® b ä¸æŸå¤±çš„å…³ç³»')
        plt.grid(alpha=0.3)

        # å­å›¾4ï¼šæ‹Ÿåˆç»“æœ
        plt.subplot(2, 2, 4)
        plt.scatter(x, y, color='skyblue', label='åŸå§‹æ•°æ®', alpha=0.6)
        w = results[opt]['w']
        b = results[opt]['b']
        plt.plot(x, w * x + b, color='crimson', linewidth=2, label=f'æ‹Ÿåˆç›´çº¿: y = {w:.2f}x + {b:.2f}')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.title(f'{opt} æ‹Ÿåˆç»“æœ')
        plt.legend()
        plt.grid(alpha=0.3)

        plt.tight_layout()
        plt.show()


def parameter_tuning_visualization(x, y):
    """å‚æ•°è°ƒèŠ‚è¿‡ç¨‹å¯è§†åŒ–ï¼šå­¦ä¹ ç‡å’Œè½®æ¬¡"""
    optimizers = ['sgd', 'adam', 'rmsprop']  # é€‰æ‹©ä¸‰ç§ä»£è¡¨æ€§ä¼˜åŒ–å™¨
    learning_rates = [0.001, 0.01, 0.1, 0.2]  # ä¸åŒå­¦ä¹ ç‡
    epochs_list = [500, 1000, 2000]  # ä¸åŒè½®æ¬¡

    # å­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
    plt.figure(figsize=(15, 10))
    for i, opt in enumerate(optimizers, 1):
        plt.subplot(1, len(optimizers), i)
        for lr in learning_rates:
            _, _, _, loss_hist, _, _ = train_model(x, y, opt, learning_rate=lr, epochs=1000)
            plt.plot(range(1000), loss_hist, label=f'å­¦ä¹ ç‡={lr}', linewidth=1.5)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('æŸå¤±å€¼')
        plt.title(f'{opt} ä¸åŒå­¦ä¹ ç‡çš„æŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.show()

    # è½®æ¬¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
    plt.figure(figsize=(15, 10))
    for i, opt in enumerate(optimizers, 1):
        plt.subplot(1, len(optimizers), i)
        for epochs in epochs_list:
            _, _, _, loss_hist, _, _ = train_model(x, y, opt, learning_rate=0.01, epochs=epochs)
            plt.plot(range(epochs), loss_hist, label=f'è½®æ¬¡={epochs}', linewidth=1.5)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('æŸå¤±å€¼')
        plt.title(f'{opt} ä¸åŒè½®æ¬¡çš„æŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.show()


def main():
    # è¯»å–æ•°æ®ï¼ˆå°è¯•ä»å½“å‰ç›®å½•æŸ¥æ‰¾train.csvï¼‰
    csv_path = r"D:\åªæƒ³ç¡è§‰\ä¸‹è½½\train.csv" # ä¿®æ”¹ä¸ºç¾¤é‡Œå‘çš„train.csvè·¯å¾„

    # å¦‚æœå½“å‰ç›®å½•æ‰¾ä¸åˆ°ï¼Œå°è¯•å…¶ä»–å¸¸è§è·¯å¾„
    if not os.path.exists(csv_path):
        # å°è¯•ç”¨æˆ·ä¸»ç›®å½•
        home_dir = os.path.expanduser("~")
        csv_path = os.path.join(home_dir, "train.csv")
        if not os.path.exists(csv_path):
            # å°è¯•æ¡Œé¢
            desktop = os.path.join(home_dir, "Desktop", "train.csv")
            if os.path.exists(desktop):
                csv_path = desktop
            else:
                print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ train.csv")
                print(f"å½“å‰ä»£ç è¿è¡Œè·¯å¾„ï¼š{os.getcwd()}")
                return

    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… æˆåŠŸè¯»å–æ•°æ®ï¼š{csv_path}ï¼Œå…± {len(df)} æ¡è®°å½•")
        print(f"CSVæ–‡ä»¶åˆ—åï¼š{df.columns.tolist()}")

        required_columns = ['x', 'y']
        # å°è¯•è‡ªåŠ¨æ£€æµ‹ç±»ä¼¼çš„åˆ—å
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸ è­¦å‘Šï¼šCSVæœªæ‰¾åˆ°åˆ— {required_columns}ï¼Œå°è¯•è‡ªåŠ¨åŒ¹é…...")
            # å¯»æ‰¾æœ€ç›¸ä¼¼çš„åˆ—å
            for col in required_columns:
                found = False
                for df_col in df.columns:
                    if col in df_col.lower():
                        required_columns[required_columns.index(col)] = df_col
                        found = True
                        print(f"  è‡ªåŠ¨åŒ¹é…ï¼š{col} -> {df_col}")
                if not found:
                    print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä¸ {col} ç›¸ä¼¼çš„åˆ—")
                    return

        df = df[required_columns].dropna()
        x = df[required_columns[0]].values
        y = df[required_columns[1]].values

        if len(x) == 0 or len(y) == 0:
            print("âŒ é”™è¯¯ï¼šxæˆ–yåˆ—ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return

        print(f"æ•°æ®èŒƒå›´ - x: [{np.min(x):.2f}, {np.max(x):.2f}], y: [{np.min(y):.2f}, {np.max(y):.2f}]")

    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥ï¼š{str(e)}")
        return

    # å®šä¹‰è¦æ¯”è¾ƒçš„ä¼˜åŒ–å™¨åˆ—è¡¨
    optimizers_to_compare = [
        'sgd', 'adagrad', 'adam',
        'adamax', 'asgd', 'rmsprop',
        'rprop', 'lbfgs'
    ]

    # æ¯”è¾ƒæ‰€æœ‰ä¼˜åŒ–å™¨
    print("\nğŸ“Œ å¼€å§‹æ¯”è¾ƒæ‰€æœ‰ä¼˜åŒ–å™¨...")
    results = compare_optimizers(x, y, optimizers_to_compare, learning_rate=0.01, epochs=1000)

    # æ‰¾åˆ°æ€§èƒ½æœ€å¥½çš„ä¼˜åŒ–å™¨ï¼ˆæŸå¤±æœ€å°ï¼‰
    best_optimizer = min(optimizers_to_compare,
                         key=lambda opt: results[opt]['loss_history'][-1])
    print(f"\nğŸ† æ€§èƒ½æœ€å¥½çš„ä¼˜åŒ–å™¨æ˜¯ï¼š{best_optimizer}ï¼Œæœ€ç»ˆæŸå¤±ï¼š{results[best_optimizer]['loss_history'][-1]:.6f}")

    # å‚æ•°è°ƒèŠ‚å¯è§†åŒ–
    print("\nğŸ“Š å¼€å§‹å‚æ•°è°ƒèŠ‚å¯è§†åŒ–...")
    parameter_tuning_visualization(x, y)


if __name__ == "__main__":
    main()
