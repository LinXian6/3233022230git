import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False


class LinearRegression:
    """çº¿æ€§å›å½’æ¨¡å‹ y = wx + b"""

    def __init__(self, learning_rate=0.001, epochs=1000):  # è°ƒæ•´äº†é»˜è®¤å­¦ä¹ ç‡
        self.w = np.random.randn()  # éšæœºåˆå§‹åŒ–æƒé‡
        self.b = np.random.randn()  # éšæœºåˆå§‹åŒ–åç½®
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.loss_history = []  # æŸå¤±å†å²
        self.w_history = []  # æƒé‡å†å²
        self.b_history = []  # åç½®å†å²

    def compute_loss(self, x, y):
        """è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±"""
        n = len(y)
        y_pred = self.w * x + self.b
        loss = np.sum((y_pred - y) ** 2) / n
        return loss

    def gradient_descent(self, x, y):
        """æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°"""
        n = len(y)
        y_pred = self.w * x + self.b
        # è®¡ç®—æ¢¯åº¦
        dw = (2 / n) * np.sum((y_pred - y) * x)
        db = (2 / n) * np.sum(y_pred - y)
        # æ›´æ–°å‚æ•°
        self.w -= self.learning_rate * dw
        self.b -= self.learning_rate * db

    def train(self, x, y):
        """è®­ç»ƒæ¨¡å‹"""
        # æ ‡å‡†åŒ–æ•°æ®ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        x_mean, x_std = np.mean(x), np.std(x)
        y_mean, y_std = np.mean(y), np.std(y)
        x_normalized = (x - x_mean) / x_std
        y_normalized = (y - y_mean) / y_std

        for epoch in range(self.epochs):
            # è®°å½•å½“å‰çŠ¶æ€
            current_loss = self.compute_loss(x_normalized, y_normalized)
            self.loss_history.append(current_loss)
            self.w_history.append(self.w)
            self.b_history.append(self.b)

            # æ¢¯åº¦ä¸‹é™æ›´æ–°
            self.gradient_descent(x_normalized, y_normalized)

            # æ¯100è½®æ‰“å°ä¿¡æ¯
            if (epoch + 1) % 100 == 0:
                print(f"è½®æ¬¡ {epoch + 1}/{self.epochs}, æŸå¤±: {current_loss:.6f}, w: {self.w:.6f}, b: {self.b:.6f}")

        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å†å²è®°å½•é•¿åº¦
        print(f"\nè®­ç»ƒè¿‡ç¨‹è®°å½•ï¼š")
        print(f"æŸå¤±è®°å½•é•¿åº¦: {len(self.loss_history)}")
        print(f"æƒé‡è®°å½•é•¿åº¦: {len(self.w_history)}")
        print(f"åç½®è®°å½•é•¿åº¦: {len(self.b_history)}")

        # è½¬æ¢å›åŸå§‹æ•°æ®å°ºåº¦çš„å‚æ•°
        self.w_original = self.w * (y_std / x_std)
        self.b_original = (self.b * y_std) + y_mean - (self.w * (y_std / x_std) * x_mean)

        return self.w_original, self.b_original


def main():
    # 1. è¯»å–æ•°æ®
    # è¯·ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„
    csv_path = r"D:\åªæƒ³ç¡è§‰\ä¸‹è½½\train.csv"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {csv_path}")
        print(f"å½“å‰ä»£ç è¿è¡Œè·¯å¾„ï¼š{os.getcwd()}")
        return

    try:
        # è¯»å–CSV
        df = pd.read_csv(csv_path)
        print(f"âœ… æˆåŠŸè¯»å–æ•°æ®ï¼š{csv_path}ï¼Œå…± {len(df)} æ¡è®°å½•")
        print(f"CSVæ–‡ä»¶åˆ—åï¼š{df.columns.tolist()}")

        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['x', 'y']
        if not all(col in df.columns for col in required_columns):
            print(f"âŒ é”™è¯¯ï¼šCSVéœ€åŒ…å«åˆ— {required_columns}ï¼Œå½“å‰åˆ—åæ˜¯ {df.columns.tolist()}")
            return

        # æå–xå’Œyå¹¶å»é™¤å¯èƒ½çš„ç¼ºå¤±å€¼
        df = df[required_columns].dropna()
        x = df['x'].values
        y = df['y'].values

        if len(x) == 0 or len(y) == 0:
            print("âŒ é”™è¯¯ï¼šxæˆ–yåˆ—ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return

        print(f"æ•°æ®èŒƒå›´ - x: [{np.min(x):.2f}, {np.max(x):.2f}], y: [{np.min(y):.2f}, {np.max(y):.2f}]")

    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥ï¼š{str(e)}")
        return

    # 2. è®­ç»ƒæ¨¡å‹
    model = LinearRegression(learning_rate=0.01, epochs=1000)
    print("\nğŸ“Œ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    w, b = model.train(x, y)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆå‚æ•°ï¼šw = {w:.6f}, b = {b:.6f}")

    # 3. ç»˜åˆ¶ç»“æœ
    plt.figure(figsize=(15, 12))

    # å­å›¾1ï¼šæ•°æ®ç‚¹+æ‹Ÿåˆç›´çº¿
    plt.subplot(2, 2, 1)
    plt.scatter(x, y, color='skyblue', label='åŸå§‹æ•°æ®', alpha=0.6)
    plt.plot(x, w * x + b, color='crimson', linewidth=2, label=f'æ‹Ÿåˆç›´çº¿: y = {w:.2f}x + {b:.2f}')
    plt.xlabel('x', fontsize=10)
    plt.ylabel('y', fontsize=10)
    plt.title('æ•°æ®åˆ†å¸ƒä¸çº¿æ€§æ‹Ÿåˆ', fontsize=12)
    plt.legend()

    # å­å›¾2ï¼šwä¸losså…³ç³»
    plt.subplot(2, 2, 2)
    if len(model.w_history) > 0 and len(model.loss_history) > 0:
        plt.plot(model.w_history, model.loss_history, color='forestgreen', linewidth=1.5)
        plt.xlabel('æƒé‡ w', fontsize=10)
        plt.ylabel('æŸå¤±å€¼', fontsize=10)
        plt.title('æƒé‡ w ä¸æŸå¤±çš„å˜åŒ–å…³ç³»', fontsize=12)
    else:
        plt.text(0.5, 0.5, 'æ— æƒé‡å†å²æ•°æ®', ha='center', va='center', transform=plt.gca().transAxes)

    # å­å›¾3ï¼šbä¸losså…³ç³»
    plt.subplot(2, 2, 3)
    if len(model.b_history) > 0 and len(model.loss_history) > 0:
        plt.plot(model.b_history, model.loss_history, color='darkorchid', linewidth=1.5)
        plt.xlabel('åç½® b', fontsize=10)
        plt.ylabel('æŸå¤±å€¼', fontsize=10)
        plt.title('åç½® b ä¸æŸå¤±çš„å˜åŒ–å…³ç³»', fontsize=12)
    else:
        plt.text(0.5, 0.5, 'æ— åç½®å†å²æ•°æ®', ha='center', va='center', transform=plt.gca().transAxes)

    # å­å›¾4ï¼šæŸå¤±éšè½®æ¬¡å˜åŒ–
    plt.subplot(2, 2, 4)
    if len(model.loss_history) > 0:
        plt.plot(range(len(model.loss_history)), model.loss_history, color='darkorange', linewidth=1.5)
        plt.xlabel('è®­ç»ƒè½®æ¬¡', fontsize=10)
        plt.ylabel('æŸå¤±å€¼', fontsize=10)
        plt.title('æŸå¤±å€¼éšè®­ç»ƒè½®æ¬¡ä¸‹é™è¶‹åŠ¿', fontsize=12)
    else:
        plt.text(0.5, 0.5, 'æ— æŸå¤±å†å²æ•°æ®', ha='center', va='center', transform=plt.gca().transAxes)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()
