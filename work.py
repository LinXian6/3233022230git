import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


# è¯»å–æ•°æ®é›†
try:
    data = pd.read_csv('countries.csv')
    print(f"âœ… æˆåŠŸè¯»å–æ•°æ®é›†ï¼Œå…± {len(data)} è¡ŒåŸå§‹æ•°æ®")
except FileNotFoundError:
    raise FileNotFoundError("âŒ æœªæ‰¾åˆ° countries.csv æ–‡ä»¶ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼")


# å®šä¹‰æ•°æ®é›†ç±»ï¼ˆå«ç¼ºå¤±å€¼åˆ é™¤+æ•°æ®ç±»å‹è½¬æ¢ï¼‰
class MyDataset(Dataset):
    def __init__(self, data):
        # åˆ é™¤åŒ…å« nan å€¼çš„è¡Œ
        self.data = data.dropna()
        print(f"âœ… åˆ é™¤ç¼ºå¤±å€¼åï¼Œå‰©ä½™æœ‰æ•ˆæ•°æ® {len(self.data)} è¡Œ")

        # ç‰¹å¾åˆ—ï¼ˆ7ä¸ªè¾“å…¥ç‰¹å¾ï¼‰å’Œç›®æ ‡åˆ—ï¼ˆæ€»ç”Ÿæ€è¶³è¿¹ï¼‰
        self.feat_cols = ['Population (millions)', 'HDI', 'Cropland Footprint',
                          'Grazing Footprint', 'Forest Footprint', 'Carbon Footprint',
                          'Fish Footprint']
        self.x = self.data[self.feat_cols].values
        self.y = self.data['Total Ecological Footprint'].values

        # è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼ˆfloat32ç±»å‹ï¼Œé¿å…ç²¾åº¦é—®é¢˜ï¼‰
        self.x = torch.tensor(self.x, dtype=torch.float32)
        self.y = torch.tensor(self.y, dtype=torch.float32).unsqueeze(1)  # æ‰©å±•ä¸ºåˆ—å‘é‡

        # è¾“å‡ºæ•°æ®åŸºæœ¬ä¿¡æ¯ï¼Œæ–¹ä¾¿æ’æŸ¥å¼‚å¸¸
        print(f"ğŸ“Š ç‰¹å¾æ•°æ®å½¢çŠ¶: {self.x.shape}, ç›®æ ‡æ•°æ®å½¢çŠ¶: {self.y.shape}")
        print(f"ğŸ“ˆ ç›®æ ‡å€¼èŒƒå›´: {self.y.min().item():.2f} ~ {self.y.max().item():.2f}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]


# å®ä¾‹åŒ–æ•°æ®é›†å¹¶åˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆ8:2åˆ†å‰²ï¼‰
dataset = MyDataset(data)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# æ•°æ®åŠ è½½å™¨ï¼ˆæ‰¹é‡å¤„ç†+æ‰“ä¹±ï¼‰
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)
print(f"ğŸ”§ è®­ç»ƒé›†æ‰¹æ¬¡æ•°é‡: {len(train_loader)} (æ¯æ‰¹16ä¸ªæ ·æœ¬)")
print(f"ğŸ”§ æµ‹è¯•é›†æ‰¹æ¬¡æ•°é‡: {len(test_loader)} (æ¯æ‰¹16ä¸ªæ ·æœ¬)")



class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # è¾“å…¥å±‚(7)â†’éšè—å±‚1(7)â†’éšè—å±‚2(6)â†’éšè—å±‚3(5)â†’è¾“å‡ºå±‚(1)
        self.fc1 = nn.Linear(7, 7)  # ç¬¬1å±‚ï¼š7è¾“å…¥â†’7è¾“å‡º
        self.fc2 = nn.Linear(7, 6)  # ç¬¬2å±‚ï¼š7è¾“å…¥â†’6è¾“å‡º
        self.fc3 = nn.Linear(6, 5)  # ç¬¬3å±‚ï¼š6è¾“å…¥â†’5è¾“å‡º
        self.fc4 = nn.Linear(5, 1)  # è¾“å‡ºå±‚ï¼š5è¾“å…¥â†’1è¾“å‡ºï¼ˆå›å½’ä»»åŠ¡æ— æ¿€æ´»ï¼‰
        self.relu = nn.ReLU()  # ReLUæ¿€æ´»å‡½æ•°ï¼ˆç»Ÿä¸€å®šä¹‰ï¼Œé¿å…é‡å¤ï¼‰

    def forward(self, x):
        # å‰å‘ä¼ æ’­ï¼šè¾“å…¥â†’ReLUâ†’è¾“å…¥â†’ReLUâ†’è¾“å…¥â†’ReLUâ†’è¾“å‡º
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.fc4(x)
        return x



# å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰ã€ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
model = Net()
criterion = nn.MSELoss()  # å›å½’ä»»åŠ¡å¸¸ç”¨MSEæŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adamä¼˜åŒ–å™¨ï¼ˆæ¯”SGDæ›´ç¨³å®šï¼‰

# è®­ç»ƒå‚æ•°
num_epochs = 100
train_losses = []  # å­˜å‚¨è®­ç»ƒæŸå¤±
test_losses = []  # å­˜å‚¨æµ‹è¯•æŸå¤±
best_loss = float('inf')  # è®°å½•æœ€ä½³æµ‹è¯•æŸå¤±ï¼ˆç”¨äºä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼‰

# åˆå§‹åŒ–Matplotlibå›¾åƒï¼ˆè®¾ç½®æ ·å¼ï¼Œæ”¯æŒå®æ—¶æ›´æ–°ï¼‰
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # æ”¯æŒè‹±æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # æ”¯æŒè´Ÿå·æ˜¾ç¤º
fig, ax = plt.subplots(figsize=(10, 6))  # è®¾ç½®å›¾åƒå¤§å°
ax.set_xlabel('Epoch', fontsize=12)
ax.set_ylabel('MSE Loss', fontsize=12)
ax.set_title('Training vs Test Loss (5-Layer Neural Network)', fontsize=14, pad=20)
ax.grid(True, alpha=0.3)  # æ·»åŠ ç½‘æ ¼çº¿ï¼Œæ–¹ä¾¿è¯»æ•°

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    model.train()  # å¼€å¯è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨Dropoutç­‰ï¼Œæ­¤å¤„æ— ä½†è§„èŒƒä¿ç•™ï¼‰
    running_train_loss = 0.0
    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):
        # æ¢¯åº¦æ¸…é›¶â†’å‰å‘ä¼ æ’­â†’è®¡ç®—æŸå¤±â†’åå‘ä¼ æ’­â†’å‚æ•°æ›´æ–°
        optimizer.zero_grad()
        y_pred = model(x_batch)
        loss = criterion(y_pred, y_batch)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()  # ç´¯åŠ æ‰¹æ¬¡æŸå¤±

    # è®¡ç®—æœ¬è½®è®­ç»ƒå¹³å‡æŸå¤±
    avg_train_loss = running_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # -------------------------- æµ‹è¯•é˜¶æ®µ --------------------------
    model.eval()  # å¼€å¯è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutç­‰ï¼‰
    running_test_loss = 0.0
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿå¹¶é¿å…å†…å­˜å ç”¨
        for x_batch, y_batch in test_loader:
            y_pred = model(x_batch)
            loss = criterion(y_pred, y_batch)
            running_test_loss += loss.item()

    # è®¡ç®—æœ¬è½®æµ‹è¯•å¹³å‡æŸå¤±
    avg_test_loss = running_test_loss / len(test_loader)
    test_losses.append(avg_test_loss)

    # -------------------------- ä¿å­˜æœ€ä¼˜æ¨¡å‹ --------------------------
    if avg_test_loss < best_loss:
        best_loss = avg_test_loss
        torch.save(model.state_dict(), 'best_model.pt')
        print(f"ğŸ“Œ Epoch {epoch}: æµ‹è¯•æŸå¤±ä¸‹é™è‡³ {best_loss:.4f}ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹")


    if epoch % 10 == 0:
        print(f"Epoch [{epoch:3d}/{num_epochs}]: "
              f"Train Loss = {avg_train_loss:.4f}, "
              f"Test Loss = {avg_test_loss:.4f}")

    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:
        ax.clear()  # æ¸…ç©ºä¸Šä¸€è½®å›¾åƒ
        # ç»˜åˆ¶è®­ç»ƒ/æµ‹è¯•æŸå¤±æ›²çº¿ï¼ˆæ·»åŠ æ›²çº¿æ ‡ç­¾ï¼‰
        ax.plot(range(1, epoch + 2), train_losses,
                label='Train Loss', color='#2E86AB', linewidth=2.5, marker='o', markersize=3)
        ax.plot(range(1, epoch + 2), test_losses,
                label='Test Loss', color='#A23B72', linewidth=2.5, marker='s', markersize=3)
        # æ ‡æ³¨æœ€ä½³æµ‹è¯•æŸå¤±ç‚¹
        best_epoch = test_losses.index(best_loss) + 1
        ax.scatter(best_epoch, best_loss, color='red', s=80, zorder=5,
                   label=f'Best Test Loss: {best_loss:.4f} (Epoch {best_epoch})')
        # é‡æ–°è®¾ç½®æ ‡ç­¾å’Œç½‘æ ¼
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('MSE Loss', fontsize=12)
        ax.set_title('Training vs Test Loss (5-Layer Neural Network)', fontsize=14, pad=20)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=10)
        plt.pause(0.1)  # æš‚åœ0.1ç§’ï¼Œè®©å›¾åƒæ›´æ–°


plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«æˆªæ–­
plt.savefig('training_test_loss.png', dpi=300, bbox_inches='tight')  # é«˜åˆ†è¾¨ç‡ä¿å­˜
plt.show()


print("\n" + "=" * 50)
print("è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_losses[-1]:.4f}")
print(f"ğŸ† æœ€ä½³æµ‹è¯•æŸå¤±: {best_loss:.4f} (å¯¹åº”Epoch {best_epoch})")
print(f"ğŸ’¾ æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: best_model.pt")
print(f"ğŸ’¾ æŸå¤±å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³: training_test_loss.png")
print("=" * 50)